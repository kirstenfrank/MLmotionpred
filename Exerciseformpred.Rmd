---
title: "Prediction of exercise form"
author: "Kirsten Frank"
date: "November 23, 2014"
output:
  html_document:
    keep_md: yes
---
references:
- id: ugulino2012
  title: Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements
  author:
  - family: Ugulino
    given: W.
  - family: Cardador
    given: D.
  - family: Vega
    given: K.
  - family: Velloso
    given: E.
  - family: Milidiu
    given: R.
  - family Fuks
    given: H.
  container-title: Lecture Notes in Computer Science
  DOI: 10.1007/978-3-642-34459-6_6
  issued:
    year: 2012
  page: 52-61

- id: breiman2001
  title: Random forests
  author: 
  - family: Breiman
    given: Leo
  container-title: Machine Learning
  issued: 
    year: 2001
  page: 5-32

---
# Summary

Machine learning was used to evaluate the feasibility of qualitatively assessing exercise quality in real time. The high accuracy of classification of exercise types demonstrated the fruitfulness of this approach.  

# Background

The use of inexpensive motion sensors, accelerometers, magnetometers,  and other devices promises to revolutionize exercise coaching. This devices allow a coach to examine fine details of the exerciser's position, motion and timing. Other devices can be attached to the exercise equipment and measure the position, motion and timing of the equipment. This data requires a lot of processing before it can interpreted. 

There are two proposals for methods to do this processing. One is to process into a physical representation of the motion and compare it to an idealized model of the exercise. The other is to measure people doing the exercise and let computers find the pattern and make a model using machine learning. 

# Methods

## Data collection
Data collected on six male volunteers each doing a dumbbell curl in five different ways. Four of these ways are mistakes that a coach would correct and the other is the correct way. 

The data was made available freely, thanks to researchers. Data is kindly supplied by http://groupware.les.inf.puc-rio.br/har with citations to their paper [@ugulino2012].

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3Ibz6YXYU

```{r download data,cache=TRUE}
URLtrain<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(URLtrain,destfile="training.csv",method="curl")
trainset<-read.csv("training.csv",stringsAsFactors=FALSE)

URLtest<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(URLtest,destfile="testing.csv",method="curl")
testset<-read.csv("testing.csv",stringsAsFactors=FALSE)
```
## Data processing
In order to use machine learning on this corpus of data, we need to divide it into a training set and a validation set. The purpose of a validation set is to allow us to estimate out-of-sample error. 40% of the observations were put into the validation set, and the remaining 60% were used to train the model.

```{r division into training and validation sets,cache=TRUE}
library(caret)
set.seed(1111)
inTrain<-createDataPartition(trainset$classe,p=0.6,list=FALSE)
MLtrain<-trainset[inTrain,]
MLvalid<-trainset[-inTrain,]
```

The data have `r ncol(trainset)` columns. Many of them are summary columns and don't contain values at every point. In order to use this data simply and effectively, columns that contained mostly NAs or blank characters were removed.

```{r removal of NA columns,cache=TRUE}
temp <- MLtrain[, which(as.numeric(colSums(is.na(MLtrain))) < 500)]
lowVar<-nearZeroVar(temp)  

    # This identifies variables with little variance.
    # Few observations in low frequency classes can cause a problem
    # in cross-validation methods. These tend to DIV by 0 errors.
    # Column 6 is a Yes/No factor variable
    # Keep column 6 but remove all the others in lowVar
lowVar<-lowVar[-1]
temp<-temp[,-lowVar]
```

Several other columns contained information that was specific to the experiment and unlikely to be generalizable, such as the datestamp and elapsed time. The X variable turned out to be an observation number (like a row number) and not generalizable. All of these were removed. The spelling error of picth instead of pitch was corrected and column names were saved to be used to remove columns from the validation set and the test set.

```{r remove timestamps,cache=TRUE}
names(temp)<-gsub("picth","pitch",names(temp))
temp$raw_timestamp_part_1<-NULL
temp$raw_timestamp_part_2<-NULL
temp$cvtd_timestamp<-NULL
temp$X<-NULL      ## this column is a row number
keepnames<-names(temp)
```

The user name was kept because the test set is a variety of exercises by the same 6 test subjects. With that information, our implementation will recognize an exercise type by any one of the 6 test subjects. We do acknowledge that using the user name as a feature prevents generalization to an arbitary individual. Several text columns were converted to factors.

```{r convert to factors,cache=TRUE}
    temp$classe<-as.factor(temp$classe)
    temp$user_name<-as.factor(temp$user_name)
    temp$new_window<-as.factor(temp$new_window)
```

## Data exploration

In order to interpret the results of machine learning, we did not plan to use principal component analysis. This meant that we could explore the remaining `r ncol(temp)-1` features using their original names.

Preliminary plots comparing pairs of features were generated from the data. Data was chosen every 150 rows, so as to avoid overplotting that would obscure the data. An ideal pair of features would have points in 5 organized color groups, well separated, but no pair like that was found.

```{r exploratory graphs,cache=FALSE}
library(caret)
#featurePlot(x=temp[seq(1,nrow(temp),by=150),c("num_window",
#                                              "pitch_arm",
#                                              "roll_dumbbell",
#                                              "pitch_dumbbell",
#                                              "roll_belt",
#                                              "pitch_forearm")],
#            y=temp$classe[seq(1,nrow(temp),150)],plot="pairs") 

#featurePlot(x=temp[seq(1,nrow(temp),by=150),c("roll_forearm",
#                                              "pitch_forearm",
#                                              "yaw_forearm",
#                                              "total_accel_forearm",
#                                              "gyros_forearm_x")],
#            y=temp$classe[seq(1,nrow(temp),by=150)],plot="pairs") 
```

## Machine Learning and Prediction

The method chosen for the machine learning was Random Forests [@breiman2001]. This method is very useful for classification prediction with many predictors. It is considered to be more robust to noise than many other classification parameters. 

It is an ensemble extension of decision trees with an additional random component. Multiple trees are built using a random subset of the predictors. It can be built using k-fold cross-validation. After many attempts at training with various tunable parameters (number of trees, number of folds in the cross-validation, grid resolution for tuning the number of random predictors per tree), these parameters were chosen (number of trees=350, number of folds=10, grid resolution=3 divisions). Numerical variables were centered and scaled in preprocessing. 

```{r machine learning,cache=TRUE}
    fitControl<-trainControl(method="repeatedcv",
                             number=10,
                             repeats=10)
    randforestfit<-train(classe~.,temp,
                         method="rf", 
                         ntree=350,
                         preProc=c("center","scale"),
                         tuneLength =3,
                         trControl=fitControl)
```
# Results and Conclusion

The resulting model fit had an in-training-set accuracy of `r round(randforestfit$results$Accuracy[2],4)`. We then evaluated the influence of the various predictors.

```{r Predictor influence, cache=TRUE}
influence<-varImp(randforestfit)
influence[1]
```

We used the list of retained columns to prepare the validation set for prediction. Prediction on the validation set allows us to calculate an out-of-sample error rate.

```{r prepare validation set,cache=TRUE}
# make MLvalid same as MLtrain (factors, remove columns)
    names(MLvalid)<-gsub("picth","pitch",names(MLvalid))
    validset<-MLvalid[,which(colnames(MLvalid) %in% keepnames)]
    validset$classe<-as.factor(validset$classe)
    validset$user_name<-as.factor(validset$user_name)
    validset$new_window<-as.factor(validset$new_window)
```

```{r predict on validation set,cache=TRUE}
# do the prediction on validation set
    prediction<-predict(randforestfit,validset)
    validationnumbers<-confusionMatrix(prediction,validset$classe)
```

Overall accuracy on the validation set (out-of-sample accuracy) was `r round(validationnumbers$overall[1],3)`. The accuracy by class was `r round(validationnumbers$byClass[1,8],3)` for class A (the correctly performed exercise); `r round(validationnumbers$byClass[2,8],3)` for class B; `r round(validationnumbers$byClass[3,8],3)` for class C; `r round(validationnumbers$byClass[4,8],3)` for class D; and `r round(validationnumbers$byClass[5,8],3)` for class E.

In conclusion, machine learning using Random Forests works well for this type of data. 

# References
references:
- id: ugulino2012
  title: Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements
  author:
  - family: Ugulino
    given: W.
  - family: Cardador
    given: D.
  - family: Vega
    given: K.
  - family: Velloso
    given: E.
  - family: Milidiu
    given: R.
  - family Fuks
    given: H.
  container-title: Lecture Notes in Computer Science
  DOI: 10.1007/978-3-642-34459-6_6
  issued:
    year: 2012
  page: 52-61

- id: breiman2001
  title: Random forests
  author: 
  - family: Breiman
    given: Leo
  container-title: Machine Learning
  issued: 
    year: 2001
  page: 5-32
