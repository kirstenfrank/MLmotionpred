---
title: "Prediction of exercise form"
author: "Kirsten Frank"
date: "November 22, 2014"
output: html_document
---
Summary
__________________________________________________________________


# Background
The use of inexpensive motion sensors, accelerometers, magnetometers,  and other devices promises to revolutionize exercise coaching. This devices allow a coach to examine fine details of the exerciser's position, motion and timing. Other devices can be attached to the exercise equipment and measure the position, motion and timing of the equipment. This data requires a lot of processing before it can interpreted. 

There are two proposals for methods to do this processing. One is to process into a physical representation of the motion and compare it to an idealized model of the exercise. The other is to measure people doing the exercise and let computers find the pattern and make a model using machine learning. 

# Methods
## Data collection
Data collected on six male volunteers each doing a dumbbell curl in five different ways. Four of these ways are mistakes that a coach would correct and the other is the correct way. 

The data was made available freely, thanks to researchers.

```{r download data}
URLtrain<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(URLtrain,destfile="training.csv",method="curl")
trainset<-read.csv("training.csv",stringsAsFactors=FALSE)

URLtest<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(URLtest,destfile="testing.csv",method="curl")
testset<-read.csv("testing.csv",stringsAsFactors=FALSE)
```
## Data processing
In order to use machine learning on this corpus of data, we need to divide it into a training set and a validation set. The purpose of a validation set is to allow us to estimate out-of-sample error. 40% of the observations were put into the validation set, and the remaining 60% were used to train the model.

```{r division into training and validation sets}
library(caret)
set.seed(1111)
inTrain<-createDataPartition(trainset$classe,p=0.6,list=FALSE)
MLtrain<-trainset[inTrain,]
MLvalid<-trainset[-inTrain,]
```

The data have `r ncol(trainset)` columns. Many of them are summary columns and don't contain values at every point. In order to use this data simply and effectively, columns that contained mostly NAs or blank characters were removed.

```{r removal of NA columns}
temp <- MLtrain[, which(as.numeric(colSums(is.na(MLtrain))) < 500)]
lowVar<-nearZeroVar(temp)  

    # This identifies variables with little variance.
    # Few observations in low frequency classes can cause a problem
    # in cross-validation methods. These tend to DIV by 0 errors.
    # Column 6 is a Yes/No factor variable
    # Keep column 6 but remove all the others in lowVar
lowVar<-lowVar[-1]
temp<-temp[,-lowVar]
```

Several other columns contained information that was specific to the experiment and unlikely to be generalizable, such as the datestamp and elapsed time. The X variable turned out to be an observation number (like a row number) and not generalizable. All of these were removed. The spelling error of picth instead of pitch was corrected and column names were saved to be used to remove columns from the validation set and the test set.

```{r remove timestamps}
names(temp)<-gsub("picth","pitch",names(temp))
temp$raw_timestamp_part_1<-NULL
temp$raw_timestamp_part_2<-NULL
temp$cvtd_timestamp<-NULL
temp$X<-NULL      ## this column is a row number
keepnames<-names(temp)
```

The user name was kept because the test set is a variety of exercises by the same 6 test subjects. With that information, our implementation will recognize an exercise type by any one of the 6 test subjects. We do acknowledge that using the user name as a feature prevents generalization to an arbitary individual. Several text columns were converted to factors.

```{r convert to factors}
    temp$classe<-as.factor(temp$classe)
    temp$user_name<-as.factor(temp$user_name)
    temp$new_window<-as.factor(temp$new_window)
```

## Data exploration
